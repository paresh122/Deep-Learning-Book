{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful open-source library for numerical computation and large-scale machine learning. It provides the backend support for building and training deep learning models. In this code, TensorFlow is the main framework that supports all subsequent operations related to constructing and training the MLP.The ‘Sequential’ model is a linear stack of layers in TensorFlow's Keras API. It allows for the easy creation of a neural network by stacking layers in sequence. \n",
    "Here, the ‘Sequential’ model is used to define the MLP structure, simplifying the process of layering different types of neural network layers.‘Dense’ represents a densely connected (fully connected) neural network layer within TensorFlow's Keras API. Each neuron in a dense layer receives input from all neurons in the previous layer, making it a fundamental building block of neural networks. \n",
    "In the provided code, ‘Dense’ is used to create the hidden and output layers of the MLP. Each ‘Dense’ layer is specified with its number of neurons and activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model \n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Dense(128, activation='relu', input_shape=(784,)),  # First hidden layer \n",
    "\n",
    "    Dense(64, activation='relu'),                       # Second hidden layer \n",
    "\n",
    "    Dense(10, activation='softmax')                     # Output layer \n",
    "\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line creates the first hidden layer of the MLP with 128 neurons. 'ReLU' is used as the activation function. The input shape (784) corresponds to the flattened input data (e.g., a 28x28 image).  the second hidden layer with 64 neurons is added. Using 'ReLU' activation helps introduce non-linearity, enabling the model to learn more complex patterns in the data.The output layer has 10 neurons (for a problem with 10 classes) and uses the 'softmax' activation. This layer outputs probability distributions over the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "\n",
    "model.compile( \n",
    "\n",
    "    optimizer='adam',  \n",
    "\n",
    "    loss='sparse_categorical_crossentropy',  \n",
    "\n",
    "    metrics=['accuracy'] \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled with the 'adam' optimizer and 'sparse_categorical_crossentropy' loss function, which are standard choices for multi-class classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage \n",
    "\n",
    "# X_train and y_train are your training data and labels \n",
    "\n",
    " model.fit(X_train, y_train, epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line demonstrates how to train the model on your training data (X_train, y_train) for a specified number of epochs (iterations over the entire dataset). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
