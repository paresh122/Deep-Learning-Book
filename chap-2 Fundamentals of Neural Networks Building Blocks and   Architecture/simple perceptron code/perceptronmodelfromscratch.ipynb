{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary liberary\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron: \n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000): \n",
    "\n",
    "        self.lr = learning_rate \n",
    "\n",
    "        self.n_iters = n_iterations \n",
    "\n",
    "        self.activation_function = self._unit_step_func \n",
    "\n",
    "        self.weights = None \n",
    "\n",
    "        self.bias = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y): \n",
    "\n",
    "        n_samples, n_features = X.shape \n",
    "\n",
    "        self.weights = np.zeros(n_features) \n",
    "\n",
    "        self.bias = 0 \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the perceptron's weights are initialized to zero for simplicity, and the bias is also set to zero. This initialization is a starting point for the learning process. \n",
    "The perceptron learning rule is applied here. The weights are adjusted based on the error (difference between the predicted and actual output). The learning rate (lr) controls how much the weights are adjusted during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(self.n_iters): \n",
    "\n",
    "            for idx, x_i in enumerate(X): \n",
    "\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias \n",
    "\n",
    "                y_predicted = self.activation_function(linear_output) \n",
    "\n",
    "                update = self.lr * (y[idx] - y_predicted) \n",
    "\n",
    "                self.weights += update * x_i \n",
    "\n",
    "                self.bias += update \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X): \n",
    "\n",
    "        linear_output = np.dot(X, self.weights) + self.bias \n",
    "\n",
    "        return self.activation_function(linear_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit step function is used as the activation function. It outputs 1 if the input is 0 or positive, and 0 otherwise. This is what enables the perceptron to perform binary classification.For making predictions, the perceptron computes a weighted sum of the inputs plus the bias, and then applies the activation function to this sum to determine the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unit_step_func(self, x): \n",
    "\n",
    "        return np.where(x >= 0, 1, 0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
