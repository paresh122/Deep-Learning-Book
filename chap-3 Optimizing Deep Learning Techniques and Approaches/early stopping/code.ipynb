{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.datasets import make_circles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (tf): \n",
    "\n",
    "TensorFlow, an open-source machine learning library, was developed by the Google Brain team. This powerful framework is extensively employed for constructing and training diverse machine learning models, prominently including neural networks. \n",
    "\n",
    "Keras Imports: \n",
    "\n",
    "The inclusion of Keras signifies the utilization of a high-level neural networks API in Python, designed to seamlessly run on TensorFlow. This API furnishes users with an intuitive interface for crafting and training sophisticated deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dataset with make_circles\n",
    "X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_circles` function originates from scikit-learn, a Python machine learning library. This utility is responsible for producing a synthetic dataset featuring data points organized in concentric circles. Specifically, it generates 100 samples (`n_samples`) introducing a slight level of noise (0.1) and ensuring reproducibility through a predefined random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function is also a component of scikit-learn, a Python library for machine learning. This function serves the purpose of dividing the dataset (`X` and `y`) into distinct training and testing sets. Specifically, 80% of the data is allocated for training (`X_train` and ‘y_train’), while the remaining 20% is set aside for testing (`X_test` and `y_test`). The inclusion of the `random_state` parameter ensures reproducibility in the splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential model, within the context of Keras, represents a linear stack of layers. This code segment illustrates the step-by-step creation of a neural network model.\n",
    "Dense layers, constituting fully connected components, are integral to this model. The initial layer boasts 256 neurons, employing the Rectified Linear Unit (ReLU) activation function, and anticipates input data of dimension 2. Subsequently, the second layer features a singular neuron utilizing a sigmoid activation function, designed for binary classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compilation phase is a prerequisite before model training. This entails defining essential components such as the loss function, optimizer, and metrics for performance monitoring throughout training. Specifically, in this instance, the binary_crossentropy loss function is designated for binary classification, the adam optimizer is employed, and accuracy serves as the metric to observe during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",         # Monitor the validation loss\n",
    "    min_delta=0.00001,        # Minimum change to qualify as an improvement\n",
    "    patience=20,                    # Number of epochs with no improvement to stop training\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EarlyStopping callback in Keras serves as a mechanism to conclude training when specific criteria are fulfilled. In this context, the training process halts if there is no discernible enhancement in the validation loss, as monitored by \"val_loss,\" persisting for a consecutive span of 20 epochs, as specified by the \"patience\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with early stopping\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3500, callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method is employed to train the model using the designated training data (`X_train`, `y_train`). Simultaneously, it assesses the model's performance on the testing data (`X_test`, `y_test`). The training duration is capped at a maximum of 3500 epochs, though it has the flexibility to conclude earlier should the early stopping condition be satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
